{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "NLG for hiding learning information.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "1qx0nyus3B64",
        "fNFhyrIYSH4V",
        "NuXseJpTRD8i"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tP19Amg-KGZr",
        "outputId": "ab6c9cfe-b3a1-4fcd-f3fe-0ae532031800"
      },
      "source": [
        "!pip install git+git://github.com/huggingface/transformers/\n",
        "!pip install --upgrade tensorflow\n",
        "!python -m pip install --upgrade pip\n",
        "!pip install -U sentence-transformers\n",
        "!pip install pyngrok\n",
        "!pip install flask_ngrok\n",
        "!pip install transformers torch\n",
        "!pip install cleantext"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+git://github.com/huggingface/transformers/\n",
            "  Cloning git://github.com/huggingface/transformers/ to /tmp/pip-req-build-mns5difl\n",
            "  Running command git clone -q git://github.com/huggingface/transformers/ /tmp/pip-req-build-mns5difl\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.9.0.dev0) (4.41.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.9.0.dev0) (5.4.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.9.0.dev0) (4.6.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.9.0.dev0) (2.23.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.9.0.dev0) (0.10.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.9.0.dev0) (21.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.9.0.dev0) (0.0.45)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.9.0.dev0) (3.0.12)\n",
            "Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers==4.9.0.dev0) (0.0.12)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.9.0.dev0) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.9.0.dev0) (2019.12.20)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers==4.9.0.dev0) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.9.0.dev0) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.9.0.dev0) (3.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.9.0.dev0) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.9.0.dev0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.9.0.dev0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.9.0.dev0) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.9.0.dev0) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.9.0.dev0) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.9.0.dev0) (7.1.2)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.5.0)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.36.2)\n",
            "Requirement already satisfied: keras-nightly~=2.5.0.dev in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.5.0.dev2021032900)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.7.4.3)\n",
            "Requirement already satisfied: grpcio~=1.34.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.34.1)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.19.5)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.12.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow) (1.5.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (3.3.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (57.2.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (1.32.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (1.8.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (0.4.4)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (4.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (4.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow) (4.6.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow) (3.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.5->tensorflow) (3.5.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (21.1.3)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.7/dist-packages (2.0.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.1.96)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.9.0.dev0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.41.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.10.0+cu102)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.0.12)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.9.0+cu102)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.22.2.post1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence-transformers) (3.7.4.3)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.10.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (4.6.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (5.4.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.0.45)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (21.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2019.12.20)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.0.1)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.7/dist-packages (5.0.5)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyngrok) (5.4.1)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Requirement already satisfied: flask_ngrok in /usr/local/lib/python3.7/dist-packages (0.0.25)\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.7/dist-packages (from flask_ngrok) (1.1.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from flask_ngrok) (2.23.0)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask_ngrok) (1.1.0)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask_ngrok) (2.11.3)\n",
            "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask_ngrok) (7.1.2)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask_ngrok) (1.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->Flask>=0.8->flask_ngrok) (2.0.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->flask_ngrok) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->flask_ngrok) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->flask_ngrok) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->flask_ngrok) (1.24.3)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.9.0.dev0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.9.0+cu102)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (5.4.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Requirement already satisfied: cleantext in /usr/local/lib/python3.7/dist-packages (1.1.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from cleantext) (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->cleantext) (1.15.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGtGSa9d4wCz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "227b5aec-f46d-4705-dea0-142e6a5f47f2"
      },
      "source": [
        "import pandas as pd\n",
        "import io\n",
        "import re\n",
        "import random\n",
        "import numpy as np\n",
        "import logging\n",
        "import math\n",
        "import os\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Optional\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import (\n",
        "CONFIG_MAPPING,\n",
        "MODEL_WITH_LM_HEAD_MAPPING,\n",
        "AutoConfig,\n",
        "AutoModelWithLMHead,\n",
        "GPT2LMHeadModel,\n",
        "AutoTokenizer,\n",
        "DataCollatorForLanguageModeling,\n",
        "DataCollatorForPermutationLanguageModeling,\n",
        "HfArgumentParser,\n",
        "LineByLineTextDataset,\n",
        "PreTrainedTokenizer,\n",
        "TextDataset,\n",
        "Trainer,\n",
        "TrainingArguments,\n",
        "set_seed,\n",
        ")\n",
        "model_name='bert-base-nli-mean-tokens'\n",
        "modelBert=SentenceTransformer(model_name)\n",
        "import cleantext\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from pyngrok import ngrok\n",
        "vectorizer = TfidfVectorizer()\n",
        "nltk.download('stopwords')\n"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxrF-5yFLkRR",
        "outputId": "fdff8929-b720-4962-8a54-5cf4736fae70"
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qthPj3QXKurb",
        "outputId": "c1221888-1779-4382-bfae-5a997402fa02"
      },
      "source": [
        "cd /content/drive/MyDrive/Colab Notebooks/webpage"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/webpage\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qx0nyus3B64"
      },
      "source": [
        "## Pre-Processing  dataset and Training Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2WflVojPkg9"
      },
      "source": [
        "!mkdir wikiPlots\n",
        "!unzip '/content/gdrive/MyDrive/WikiMoviePlots.zip' -d 'wikiPlots'\n",
        "import pandas as pd\n",
        "import io\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xo6DJXkQHtr"
      },
      "source": [
        "df = pd.read_csv(\"/content/wikiPlots/wiki_movie_plots_deduped.csv\", na_values = ['no info', '.'])\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "genre_selection=df.loc[df['Genre'] == \"comedy\"]\n",
        "tmp=genre_selection['Plot'].tolist()\n",
        "tmp= np.array([('<BOS> <comedy> '+xi +' <EOS>') for xi in tmp])\n",
        "np.savetxt(r'/content/wikiPlots/comedyText.txt',tmp, fmt='%s')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxI5blmXFvvi"
      },
      "source": [
        "!split -b $(expr $(cat '/content/wikiPlots/comedyText.txt' | wc -m) \\* 80 / 100) '/content/wikiPlots/comedyText.txt' 'comedy_train.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNFhyrIYSH4V"
      },
      "source": [
        "## GPT2 Story Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olhNj5FZX7ga"
      },
      "source": [
        "# import torch\n",
        "# import gc\n",
        "# torch.cuda.empty_cache()\n",
        "# gc.collect()\n",
        "# torch.cuda.memory_summary(device=None, abbreviated=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltEwIIMDg9ex"
      },
      "source": [
        "def genGpt2(f1,f2,f3,genreList,s1,s2,s3):\n",
        "  # Setup logging\n",
        "  logger = logging.getLogger(__name__)\n",
        "\n",
        "  # Get access to model types and model configs to select GPT2 model and config\n",
        "  MODEL_CONFIG_CLASSES = list(MODEL_WITH_LM_HEAD_MAPPING.keys())\n",
        "  MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\n",
        "\n",
        "\n",
        "  @dataclass\n",
        "  class ModelArguments:\n",
        "      \"\"\"\n",
        "      Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.\n",
        "      \"\"\"\n",
        "\n",
        "      model_name_or_path: Optional[str] = field(\n",
        "          default=None,\n",
        "          metadata={\n",
        "              \"help\": \"The model checkpoint for weights initialization. Leave None if you want to train a model from scratch.\"\n",
        "          },\n",
        "\n",
        "      )\n",
        "      model_type: Optional[str] = field(\n",
        "          default=None,\n",
        "          metadata={\n",
        "              \"help\": \"If training from scratch, pass a model type from the list: \"\n",
        "              + \", \".join(MODEL_TYPES)\n",
        "          },\n",
        "      )\n",
        "      cache_dir: Optional[str] = field(\n",
        "          default=None,\n",
        "          metadata={\n",
        "              \"help\": \"Where do you want to store the pretrained models downloaded from s3\"\n",
        "          },\n",
        "      )\n",
        "\n",
        "\n",
        "  @dataclass\n",
        "  class DataTrainingArguments:\n",
        "      \"\"\"\n",
        "      Arguments pertaining to what data we are going to input our model for training and eval.\n",
        "      \"\"\"\n",
        "\n",
        "      train_data_file: Optional[str] = field(\n",
        "          default=None, metadata={\"help\": \"The input training data file (a text file).\"}\n",
        "      )\n",
        "      eval_data_file: Optional[str] = field(\n",
        "          default=None,\n",
        "          metadata={\n",
        "              \"help\": \"An optional input evaluation data file to evaluate the perplexity on (a text file).\"\n",
        "          },\n",
        "      )\n",
        "      line_by_line: bool = field(\n",
        "          default=False,\n",
        "          metadata={\n",
        "              \"help\": \"Whether distinct lines of text in the dataset are to be handled as distinct sequences.\"\n",
        "          },\n",
        "      )\n",
        "\n",
        "      mlm: bool = field(\n",
        "          default=False,\n",
        "          metadata={\n",
        "              \"help\": \"Train with masked-language modeling loss instead of language modeling.\"\n",
        "          },\n",
        "      )\n",
        "\n",
        "      block_size: int = field(\n",
        "          default=-1,\n",
        "          metadata={\n",
        "              \"help\": \"Optional input sequence length after tokenization.\"\n",
        "              \"The training dataset will be truncated in block of this size for training.\"\n",
        "              \"Default to the model max input length for single sentence inputs (take into account special tokens).\"\n",
        "          },\n",
        "      )\n",
        "      overwrite_cache: bool = field(\n",
        "          default=False,\n",
        "          metadata={\"help\": \"Overwrite the cached training and evaluation sets\"},\n",
        "      )\n",
        "\n",
        "\n",
        "  # Create LineByLineDataset from Movie Plots text file\n",
        "  def get_dataset(\n",
        "      args: DataTrainingArguments, tokenizer: PreTrainedTokenizer, evaluate=False\n",
        "  ):\n",
        "      file_path = args.eval_data_file if evaluate else args.train_data_file\n",
        "      if args.line_by_line:\n",
        "          return LineByLineTextDataset(\n",
        "              tokenizer=tokenizer, file_path=file_path, block_size=args.block_size\n",
        "          )\n",
        "      else:\n",
        "          return TextDataset(\n",
        "              tokenizer=tokenizer,\n",
        "              file_path=file_path,\n",
        "              block_size=args.block_size,\n",
        "              overwrite_cache=args.overwrite_cache,\n",
        "          )\n",
        "\n",
        "  def main():\n",
        "\n",
        "      model_args = ModelArguments(\n",
        "          model_name_or_path=\"/content/drive/MyDrive/Colab Notebooks/webpage/story_generator_checkpoint\", model_type=\"gpt2-medium\"\n",
        "          # , cache_dir=\"\"\n",
        "      )\n",
        "      data_args = DataTrainingArguments(\n",
        "          train_data_file=\"/content/train_text1.txt\",\n",
        "          eval_data_file=\"/content/eval_text1.txt\",\n",
        "          line_by_line=True,\n",
        "          block_size=512,\n",
        "          overwrite_cache=True,\n",
        "      )\n",
        "      training_args = TrainingArguments(\n",
        "          output_dir=\"story_generator_checkpoint\",\n",
        "          overwrite_output_dir=True,\n",
        "          do_train=True,\n",
        "          do_eval=True,\n",
        "          # evaluate_during_training=False,\n",
        "          logging_steps=500,\n",
        "          per_device_train_batch_size=2,\n",
        "          num_train_epochs=2,\n",
        "          save_total_limit=1,\n",
        "          save_steps=1000,\n",
        "          learning_rate=2e-5,\n",
        "          per_device_eval_batch_size=2,\n",
        "          weight_decay=0.01,\n",
        "          load_best_model_at_end=True,\n",
        "      )\n",
        "\n",
        "      if data_args.eval_data_file is None and training_args.do_eval:\n",
        "          raise ValueError(\n",
        "              \"Cannot do evaluation without an evaluation data file. Either supply a file to --eval_data_file \"\n",
        "              \"or remove the --do_eval argument.\"\n",
        "          )\n",
        "\n",
        "      if (\n",
        "          os.path.exists(training_args.output_dir)\n",
        "          and os.listdir(training_args.output_dir)\n",
        "          and training_args.do_train\n",
        "          and not training_args.overwrite_output_dir\n",
        "      ):\n",
        "          raise ValueError(\n",
        "              f\"Output directory ({training_args.output_dir}) already exists and is not empty. Use --overwrite_output_dir to overcome.\"\n",
        "          )\n",
        "\n",
        "      # Setup logging\n",
        "      logging.basicConfig(\n",
        "          format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
        "          datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "          level=logging.INFO if training_args.local_rank in [-1, 0] else logging.WARN,\n",
        "      )\n",
        "      logger.warning(\n",
        "          \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
        "          training_args.local_rank,\n",
        "          training_args.device,\n",
        "          training_args.n_gpu,\n",
        "          bool(training_args.local_rank != -1),\n",
        "          training_args.fp16,\n",
        "      )\n",
        "      logger.info(\"Training/evaluation parameters %s\", training_args)\n",
        "\n",
        "      # Set seed for deterministic training runs\n",
        "      set_seed(training_args.seed)\n",
        "\n",
        "\n",
        "      config = AutoConfig.from_pretrained(\n",
        "          model_args.model_name_or_path, cache_dir=model_args.cache_dir\n",
        "      )\n",
        "\n",
        "      tokenizer = AutoTokenizer.from_pretrained(\n",
        "          model_args.model_name_or_path, cache_dir=model_args.cache_dir\n",
        "      )\n",
        "\n",
        "      model = GPT2LMHeadModel.from_pretrained(\n",
        "          model_args.model_name_or_path,\n",
        "          from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n",
        "          config=config,\n",
        "          cache_dir=model_args.cache_dir,\n",
        "      )\n",
        "\n",
        "      special_tokens_dict = {\n",
        "          \"bos_token\": \"<BOS>\",\n",
        "          \"eos_token\": \"<EOS>\",\n",
        "          \"pad_token\": \"<PAD>\",\n",
        "          \"additional_special_tokens\": [\n",
        "              \"<superhero>\",\n",
        "              \"<action>\",\n",
        "              \"<drama>\",\n",
        "              \"<thriller>\",\n",
        "              \"<horror>\",\n",
        "              \"<comedy>\",\n",
        "              \"<sci_fi>\"\n",
        "          ],\n",
        "      }\n",
        "\n",
        "      num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
        "      model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "      if data_args.block_size <= 0: \n",
        "        # If block_size <= 0, set it to max. possible value allowed by model\n",
        "          data_args.block_size = tokenizer.model_max_length\n",
        "      else:\n",
        "          data_args.block_size = min(data_args.block_size, tokenizer.model_max_length)\n",
        "\n",
        "      # Get datasets\n",
        "\n",
        "      train_dataset = (\n",
        "          get_dataset(data_args, tokenizer=tokenizer) if training_args.do_train else None\n",
        "      )\n",
        "      eval_dataset = (\n",
        "          get_dataset(data_args, tokenizer=tokenizer, evaluate=True)\n",
        "          if training_args.do_eval\n",
        "          else None\n",
        "      )\n",
        "      data_collator = DataCollatorForLanguageModeling(\n",
        "          tokenizer=tokenizer,\n",
        "          mlm=data_args.mlm,\n",
        "      )\n",
        "\n",
        "      # Initialize our Trainer\n",
        "      trainer = Trainer(\n",
        "          model=model,\n",
        "          args=training_args,\n",
        "          data_collator=data_collator,\n",
        "          train_dataset=train_dataset,\n",
        "          eval_dataset=eval_dataset,\n",
        "          # prediction_loss_only=True,\n",
        "      )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # Run these cells for story generation\n",
        "  from transformers import pipeline, TextGenerationPipeline, GPT2LMHeadModel, AutoTokenizer\n",
        "  checkpoint = \"/content/drive/MyDrive/Colab Notebooks/webpage/story_generator_checkpoint\"\n",
        "\n",
        "  model = GPT2LMHeadModel.from_pretrained(checkpoint)\n",
        "  tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "  story_generator = TextGenerationPipeline(model=model, tokenizer=tokenizer)\n",
        "  # The format for input_prompt: \"<BOS> <genre> Optional text...\"\n",
        "  # Supported genres: superhero, sci_fi, horror, thriller, action, drama , comedy\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # input_prompt = \"<BOS> <superhero> Green Arrow \"\n",
        "\n",
        "  genre=\"<BOS> \"+genreList\n",
        "  input_list=[f1,f2,f3]\n",
        "  extra=200\n",
        "  firstInput=genre+\" \"+input_list[0]\n",
        "\n",
        "  for i in range(len(input_list)):\n",
        "    if (i==0): \n",
        "      story = story_generator(firstInput, max_length=300, do_sample=True,\n",
        "                  repetition_penalty=1.2, temperature=1.5, \n",
        "                  top_p=0.95, top_k=50)\n",
        "      lst_str = str(story)[36:-2] #38\n",
        "      lst_str.replace('\\\\','')\n",
        "      lst_str.replace(':','')\n",
        "      lst_str.replace('/','')\n",
        "      lst_str.replace('[citation needed]','')\n",
        "      lst_str.replace('\\\\\\'s',\"'s\")\n",
        "      lst_str.replace(\"\\\\\\\\\\\\'s\",\"'s\")\n",
        "      lst_str.replace(\"\\\\'s\",\"'s\")\n",
        "      lst_str.replace(\"\\'s\",\"'s\")\n",
        "      lst_str=''.join(filter(lambda x: x!=' ’ \\n' and x!='\"\\n' and x!='\\n' and x!='’\\n' and x!=' \\n' and x!=\"\\\\\\\\\\\\\", lst_str))\n",
        "    else:\n",
        "      extra+=100\n",
        "      inputPrmpt=genre+\" \"+lst_str+input_list[i]\n",
        "      story = story_generator(inputPrmpt, max_length=300+extra, do_sample=True,\n",
        "                  repetition_penalty=1.2, temperature=1.5, \n",
        "                  top_p=0.95, top_k=50)\n",
        "      lst_str = str(story)[40:-3]\n",
        "      lst_str.replace('/','')\n",
        "      lst_str.replace('[citation needed]' and '\\\\\\'s' ,'')\n",
        "      lst_str.replace(\"\\\\\\\\\\\\'s\",\"'s\")\n",
        "      lst_str.replace(\"\\\\'s\",\"'s\")\n",
        "      lst_str.replace(\"\\'s\",\"'s\")\n",
        "      lst_str.replace(\"\\'\",\"'s\")\n",
        "      lst_str.replace(\"\\\\\\'\",\"\")\n",
        "      lst_str.replace(\"\\\\xa0all\",'')\n",
        "      lst_str.replace(\"xa0all\",'')\n",
        "      lst_str.replace(\"xa0w\",'')\n",
        "      lst_str.replace('\\\\\\\\\\\\','')\n",
        "      lst_str.replace(\"\\ \",\"\")\n",
        "      lst_str.replace(\"\\\\\\'t\",\"'t\")\n",
        "      # print(lst_str)\n",
        "      for i in lst_str:\n",
        "         if (lst_str.find('\\\\')!=-1 ):\n",
        "           lst_str.replace('\\\\','')\n",
        "      lst_str=''.join(filter(lambda x: x!=' ’ \\n' and x!='\"\\n' and x!='\\n' and x!='’\\n' and x!=' \\n' and x!=\"\\\\\\\\\\\\\", lst_str))\n",
        "\n",
        "\n",
        "\n",
        "    # nextStr=genre+\" \"+lst_str\n",
        "    \n",
        "  # print(lst_str,sep='\\n')\n",
        "\n",
        "\n",
        "  # story = story_generator(input_prompt, max_length=300, do_sample=True,\n",
        "  #                repetition_penalty=1.1, temperature=1.1, \n",
        "  #                top_p=0.95, top_k=50)\n",
        "\n",
        "  # print(story)\n",
        "\n",
        "\n",
        "  splitted=re.split('; |, |\\*|\\n',lst_str)\n",
        "  splitted\n",
        "\n",
        "  sentence_vec1=modelBert.encode(splitted)\n",
        "  # print(sentence_vec1)\n",
        "  encodedSentence1=[]\n",
        "  inputSentence1=[s1,s2,s3]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  i=0\n",
        "  while i<len(inputSentence1):\n",
        "    encodedSentence1.insert(i,modelBert.encode(inputSentence1[i]))\n",
        "    i+=1\n",
        "\n",
        "\n",
        "  i = 0\n",
        "  # k=0\n",
        "  list_of_probabilities1=[]\n",
        "  list_of_indicies1=[]\n",
        "  for z in range(len(inputSentence1)):\n",
        "    while i < len(splitted):\n",
        "      list_of_probabilities1.append(cosine_similarity([sentence_vec1[i]],[encodedSentence1[z]]))\n",
        "      i += 1\n",
        "    lf1 = [val for sublist in list_of_probabilities1 for val in sublist]\n",
        "    lf3=[val for sublist in lf1 for val in sublist]\n",
        "    # print(lf3)\n",
        "    x = max(lf3)\n",
        "    list_of_indicies1.append(str(list_of_probabilities1.index(x)))\n",
        "    # print(\"Highest Probability equals::\")\n",
        "    # print(x)\n",
        "    # print(\"Index::\" + str(list_of_probabilities1.index(x)))\n",
        "    # print(\"Sentence::\" + splitted[list_of_probabilities1.index(x)])\n",
        "    #re initiallizing for next sentence\n",
        "    list_of_probabilities1=[]\n",
        "    i=0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  for i in range(len(inputSentence1)):\n",
        "    splitted.insert(int(list_of_indicies1[i])+1,inputSentence1[i])\n",
        "\n",
        "\n",
        "  finalStry=' '.join(splitted)\n",
        "  return finalStry"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7aKid636X82"
      },
      "source": [
        "genGpt2('Superman wanted to save the world','Superman had to kill lex luthor','Lex luthor','<superhero>','superman was happy','superman loved his life','superman saved earth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4rXzK9I_FMD"
      },
      "source": [
        "#  # Training\n",
        "#     try:\n",
        "#       if training_args.do_train:\n",
        "#           model_path = (\n",
        "#               model_args.model_name_or_path\n",
        "#               if model_args.model_name_or_path is not None\n",
        "#               and os.path.isdir(model_args.model_name_or_path)\n",
        "#               else None\n",
        "#           )\n",
        "#           trainer.train(model_path=model_path)\n",
        "#           trainer.save_model()\n",
        "#           tokenizer.save_pretrained(training_args.output_dir)\n",
        "#     except KeyboardInterrupt:\n",
        "#       print(\"Saving model that was in the middle of training\")\n",
        "#       trainer.save_model()\n",
        "#       tokenizer.save_pretrained(training_args.output_dir)\n",
        "#       return\n",
        "\n",
        "#     # Evaluation\n",
        "#     results = {}\n",
        "#     if training_args.do_eval:\n",
        "#         logger.info(\"*** Evaluate ***\")\n",
        "\n",
        "#         eval_output = trainer.evaluate()\n",
        "\n",
        "#         perplexity = math.exp(eval_output[\"eval_loss\"])\n",
        "#         result = {\"perplexity\": perplexity}\n",
        "\n",
        "#         output_eval_file = os.path.join(training_args.output_dir, \"eval_results_lm.txt\")\n",
        "#         if trainer.is_world_process_zero():\n",
        "#             with open(output_eval_file, \"w\") as writer:\n",
        "#                 logger.info(\"***** Eval results *****\")\n",
        "#                 for key in sorted(result.keys()):\n",
        "#                     logger.info(\"  %s = %s\", key, str(result[key]))\n",
        "#                     writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
        "\n",
        "#         results.update(result)\n",
        "\n",
        "#     return results\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     main()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1Ix3AOEjsa9"
      },
      "source": [
        "\n",
        "# EMBEDDING SENTENCES IN AN EXISTING STORY\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8eRfmoDwjfMr"
      },
      "source": [
        "def gen(s1,s2,s3,s4,s5):\n",
        "  import os\n",
        "  import re\n",
        "\n",
        "  directory = '/content/drive/MyDrive/Colab Notebooks/webpage/KidsStoriesAnimals/'\n",
        "\n",
        "  TempM=[]\n",
        "  Temp=[]\n",
        "  StryName=[]\n",
        "  # i=0\n",
        "  for filename in os.listdir(directory):\n",
        "      if filename.endswith(\".txt\"):\n",
        "          TempX=[]\n",
        "          with open(\"/content/drive/MyDrive/Colab Notebooks/webpage/KidsStoriesAnimals\"+'/'+filename,\"r\") as f:\n",
        "            TempX = [line.replace('\\n','') for line in f.readlines() ]\n",
        "            TempX= [  line.split('.') for line in TempX]\n",
        "            TempX= [val for sublist in TempX for val in sublist]\n",
        "            TempX= [  line.split('?') for line in TempX]\n",
        "            TempX= [val for sublist in TempX for val in sublist]\n",
        "            TempX= list(filter(lambda x: x!=' ’ \\n' and x!='\"\\n' and x!='\\n' and x!='’\\n' and x!=' \\n', TempX))\n",
        "            \n",
        "\n",
        "          TempM.append(TempX)\n",
        "          StryName.append(filename)\n",
        "          continue\n",
        "      else:\n",
        "        continue  \n",
        "  # TempM return each story as a list where each line is a string\n",
        "  for i in TempM:\n",
        "    if (len(i)>0):\n",
        "      i.pop()\n",
        "  # print(TempM)         \n",
        "  StrpStry=[]\n",
        "  for i in range(0,4):\n",
        "    with open(\"/content/drive/MyDrive/Colab Notebooks/webpage/KidsStoriesAnimals/\"+StryName[i], \"r\") as a_file:\n",
        "        x=a_file.readlines()\n",
        "        StrpStry.append(x)\n",
        "  # print(StrpStry)  \n",
        "  k=0\n",
        "  flattened=[]\n",
        "  while k<len(TempM):\n",
        "    flattened.insert(k,TempM[k])\n",
        "\n",
        "    k+=1\n",
        "\n",
        "\n",
        "  inputSentenceFind=[s1,s2,s3,s4]\n",
        "  input_animal=s5\n",
        "\n",
        "  options=[]\n",
        "  indx_of_options=[]\n",
        "  stry_of_options=[]\n",
        "\n",
        "  for i in range(len(StryName)-1):\n",
        "    if (StryName[i].find(input_animal) != -1):\n",
        "      options.append(StryName[i])\n",
        "\n",
        "\n",
        "  for i in range(len(options)):\n",
        "    indx_of_options.append(StryName.index(options[i]))\n",
        "    stry_of_options.append(flattened[indx_of_options[i]])\n",
        "\n",
        "\n",
        "  max_of_prob=[]\n",
        "\n",
        "  def iter(w):\n",
        "    l=0\n",
        "    i = 0\n",
        "    list_of_probabilitiesFind=[]\n",
        "    list_of_indiciesFind=[]\n",
        "    list_of_summation=[]\n",
        "    Sum=0\n",
        "    encodedSentenceFind=[]\n",
        "\n",
        "\n",
        "    while l<len(inputSentenceFind):\n",
        "      encodedSentenceFind.insert(l,modelBert.encode(inputSentenceFind[l]))\n",
        "      l+=1\n",
        "\n",
        "    sentence_vecFind=modelBert.encode(stry_of_options[w]) \n",
        "    for z in range(len(inputSentenceFind)):\n",
        "      while i < len(stry_of_options[w]):\n",
        "        list_of_probabilitiesFind.append(cosine_similarity([sentence_vecFind[i]],[encodedSentenceFind[z]]))\n",
        "        i += 1\n",
        "\n",
        "        lfFind = [val for sublist in list_of_probabilitiesFind for val in sublist]\n",
        "        lf2Find=[val for sublist in lfFind for val in sublist]\n",
        "        Sum=sum(lf2Find)\n",
        "      \n",
        "      \n",
        "\n",
        "    list_of_summation.insert(i,Sum)\n",
        "    max_of_prob.append(Sum)\n",
        "    # print(list_of_summation)\n",
        "\n",
        "\n",
        "\n",
        "  for k in range(len(options)):\n",
        "    iter(k)\n",
        "      \n",
        "  # Finding the matching story name and its index in our files\n",
        "  maximum=max(max_of_prob)\n",
        "  Indx=max_of_prob.index(maximum)\n",
        "  # print('\\n')\n",
        "  # print(\"Index::\",Indx)\n",
        "  # print('\\n')\n",
        "  # print(options[Indx])   \n",
        "\n",
        "  # Finding Where to place the sentences\n",
        "  Story=stry_of_options[Indx]\n",
        "  sentence_vec=modelBert.encode(Story) \n",
        "  encodedSentence=[]\n",
        "  # inputSentence=[\"The lion is the strongest animal.\",\"The lion wieghts 40kg\",\"Nearly all wild lions live in Africa.\",\"Lions are the second largest big cat species in the world \"]\n",
        "  i=0\n",
        "  while i<len(inputSentenceFind):\n",
        "    encodedSentence.insert(i,modelBert.encode(inputSentenceFind[i]))\n",
        "    i+=1\n",
        "\n",
        "  # Finding where to place inside the story contd.\n",
        "  u = 0\n",
        "  list_of_probabilities=[]\n",
        "  list_of_indicies=[]\n",
        "  for z in range(len(inputSentenceFind)):\n",
        "    while u < len(Story):\n",
        "      list_of_probabilities.append(cosine_similarity([sentence_vec[u]],[encodedSentence[z]]))\n",
        "      u += 1\n",
        "\n",
        "    lf = [val for sublist in list_of_probabilities for val in sublist]\n",
        "    lf2=[val for sublist in lf for val in sublist]\n",
        "    x = max(lf2)\n",
        "    list_of_indicies.append(str(list_of_probabilities.index(x)))\n",
        "    # print(\"Highest Probability equals::\")\n",
        "    # print(x)\n",
        "    # print(\"Index::\" + str(list_of_probabilities.index(x)))\n",
        "    # print(\"Sentence::\" + Story[list_of_probabilities.index(x)])\n",
        "    #re initiallizing for next sentence\n",
        "    list_of_probabilities=[]\n",
        "    u=0\n",
        "\n",
        "  # Placing the Sentences and Printing the output\n",
        "  list_for_quotes=[]\n",
        "  list_for_quotes2=[]\n",
        "  StoryX=' '.join(Story)\n",
        "  matches=re.findall(r'\\“(.+?)\\”',StoryX)\n",
        "  matches2=re.findall(r'\\\"(.+?)\\\"',StoryX)\n",
        "#--------------------------------------------- \n",
        "  for j in Story:\n",
        "    for k in matches:\n",
        "      \n",
        "      if j.find(k) != -1:\n",
        "        # print(j)\n",
        "        # print(Story[3])\n",
        "        list_for_quotes.append(Story.index(j))\n",
        "# --------------------------------------------\n",
        "  for j in Story:\n",
        "    for k in matches2:\n",
        "      \n",
        "      if j.find(k) != -1:\n",
        "        list_for_quotes2.append(Story.index(j))      \n",
        "# --------------------------------------------\n",
        "  # print(matches)      \n",
        "  def cosine_sim(text1, text2):\n",
        "    tfidf = vectorizer.fit_transform([text1, text2])\n",
        "    return ((tfidf * tfidf.T).A)[0,1]  \n",
        "\n",
        "  def second_largest(numbers):\n",
        "      count = 0\n",
        "      m1 = m2 = float('-inf')\n",
        "      for x in numbers:\n",
        "          count += 1\n",
        "          if x > m2:\n",
        "              if x >= m1:\n",
        "                  m1, m2 = x, m1            \n",
        "              else:\n",
        "                  m2 = x\n",
        "      return m2 if count >= 2 else None\n",
        "\n",
        "  j=-1\n",
        "  X=[]\n",
        "  X1=[]\n",
        "  X2=[] \n",
        "  X3=[]\n",
        "  \n",
        "  array_of_indx=[]\n",
        "  array_of_indx1=[]\n",
        "  array_of_indx2=[]\n",
        "  array_of_indx3=[]\n",
        "\n",
        "\n",
        "  w0=inputSentenceFind[0]\n",
        "  w1=inputSentenceFind[1]\n",
        "  w2=inputSentenceFind[2]\n",
        "  w3=inputSentenceFind[3]\n",
        "  \n",
        "  for j in Story:\n",
        "    x0=cosine_sim(j,w0)\n",
        "    x1=cosine_sim(j,w1)\n",
        "    x2=cosine_sim(j,w2)\n",
        "    x3=cosine_sim(j,w3)\n",
        "    \n",
        "    X.append(x0)\n",
        "    X1.append(x1)\n",
        "    X2.append(x2)\n",
        "    X3.append(x3)\n",
        "\n",
        "\n",
        "  for i in X:\n",
        "    if i !=0:\n",
        "       array_of_indx.append(X.index(i))\n",
        "       \n",
        "  for i in X1:\n",
        "    if i !=0:\n",
        "       array_of_indx1.append(X1.index(i))       \n",
        "  for i in X2:\n",
        "    if i !=0:\n",
        "       array_of_indx2.append(X2.index(i))\n",
        "  for i in X3:\n",
        "    if i !=0:\n",
        "       array_of_indx3.append(X3.index(i))      \n",
        "\n",
        "  IndexOfMaximumFromX=X.index(max(X))\n",
        "  IndexOfMaximumFromX1=X1.index(max(X1))\n",
        "  IndexOfMaximumFromX2=X2.index(max(X2))\n",
        "  IndexOfMaximumFromX3=X3.index(max(X3))\n",
        "\n",
        "  \n",
        "  X=list(filter(lambda x: x!= 0, X))\n",
        "  X1=list(filter(lambda x: x!= 0, X1))\n",
        "  X2=list(filter(lambda x: x!= 0, X2))\n",
        "  X3=list(filter(lambda x: x!= 0, X3))\n",
        " \n",
        "  # print(IndexOfMaximumFromX)\n",
        "\n",
        "  scndMaxX=X.index(second_largest(X))\n",
        "  scndMaxX1=X1.index(second_largest(X1))\n",
        "  scndMaxX2=X2.index(second_largest(X2))\n",
        "  scndMaxX3=X3.index(second_largest(X3))\n",
        "  \n",
        "  randList=[IndexOfMaximumFromX,scndMaxX]\n",
        "  randList1=[IndexOfMaximumFromX1,scndMaxX1]\n",
        "  randList2=[IndexOfMaximumFromX2,scndMaxX2]\n",
        "  randList3=[IndexOfMaximumFromX3,scndMaxX3]\n",
        "  n = random.randint(0,1)\n",
        "  n1 = random.randint(0,1)\n",
        "  n2 = random.randint(0,1)\n",
        "  n3 = random.randint(0,3)\n",
        "\n",
        "# --------------------------------------------------\n",
        "  # List_of_insert_indices=[]\n",
        "  # List_of_insert_indices.append(IndexOfMaximumFromX)\n",
        "  # List_of_insert_indices.append(n)\n",
        "  # List_of_insert_indices.append(n)\n",
        "  # List_of_insert_indices.append(n)\n",
        "\n",
        "  List_of_linking_words=['as',',also','furthermore','additionally','and']\n",
        "  rand = random.randint(0,len(List_of_linking_words)-1)\n",
        "  rand1 = random.randint(0,len(List_of_linking_words)-1)\n",
        "  rand2 = random.randint(0,len(List_of_linking_words)-1)\n",
        "  rand3 = random.randint(0,len(List_of_linking_words)-1)\n",
        "\n",
        "\n",
        "  for i in range(len(inputSentenceFind)):\n",
        "    if (i==0):\n",
        "      Story.insert(randList[n]+1,(List_of_linking_words[rand]+\" \"+inputSentenceFind[i])+\".\")\n",
        "    if (i==1):\n",
        "      Story.insert(randList1[n1]+1,(List_of_linking_words[rand1]+\" \"+inputSentenceFind[i])+\".\")\n",
        "    if (i==2):\n",
        "      Story.insert(randList2[n2]+1,(List_of_linking_words[rand2]+\" \"+inputSentenceFind[i])+\".\")  \n",
        "    if (i==3):\n",
        "      Story.insert(array_of_indx3[n3]+1,(List_of_linking_words[rand3]+\" \"+inputSentenceFind[i])+\".\")  \n",
        "\n",
        "  Story=' '.join(Story)\n",
        "  return Story"
      ],
      "execution_count": 217,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cte1l28V6zOy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "cb152c10-679e-4c84-998d-f01f99f4a58d"
      },
      "source": [
        "gen('cats eat alot','cats sleep alot ','cats are friendly','cats dont eat starch','cat')"
      ],
      "execution_count": 218,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Chloe was a little girl  She had no friends but loved animals, especially cats furthermore cats are friendly. and cats eat alot.  On her tenth birthday, her father took her to an animal shelter where there were many cats  “You can chose a cat to take home, Chloe,” said her father furthermore cats dont eat starch. as cats sleep alot .  Chloe was overjoyed! She hugged her father and said, “Oh! This is my best gift ever ” Chloe liked all the cats at the shelter, but fell in love with a particular cat who had big, bright eyes  “I’ll take her,” Chloe said and named her Star  In a few days time, Star became very naughty  When Chloe was around she was good, but the moment she left for school, Star got down to her tricks  She tore the curtains, clawed the seats, lapped up all the milk in the kitchen and often stole fish  Chloe’s parents were very angry  One day, they told Chloe that they would return Star to the shelter, if she did not learn to behave Chloe was very worried, she did not know how to tame Star  Then one day, Star gave birth to kittens  They mewed and waved their tiny paws  From that day, everything changed  Star was no longer a naughty cat  She took good care of her kittens  However, Chloe’s parents were worried now that there were so many cats in the house  Every night when the kittens meowed, her father became very angry  One night  Star cried so loudly that everybody woke up  Chloe worried now that there were so many cats in the house  Every night when the kittens meowed, her father became very angry  One night, Star cried so loudly that everybody woke up  Chloe and her parents came rushing and saw Star running about frantically  Seeing Chloe, Star looked towards the kitchen and Chloe understood that her cat was trying to tell her something  Suddenly, they noticed a huge fire in the kitchen  “Hurry, let’s go out!” screamed Chloe’s father  “But what about the kittens ” asked Chloe  Chloe quickly grabbed Star and one kitten  There was no time to pick up the others and rushed out  But Star wriggled out and leapt back inside the house  Soon the fire brigade arrived  They warned everybody to stay at a safe distance from the house  “Please save my cat  She is inside!” pleaded Chloe  Suddenly, Star emerged with one kitten in her mouth  The firemen tried to stop her but she ran inside the house again  One by one she brought out six kittens, but her last baby was still inside  Star jumped inside as the flames engulfed the house  Chloe and her parents looked on anxiously  The firemen brought out their huge pipes and began spraying water  After almost an hour, the flames died  Everything was quiet, but still there was no sign of Star  Chloe cried inconsolably  She had lost her best friend  Suddenly her father exclaimed, “Look, Star!” Chloe looked up  One of the firemen emerged from the house with Star and her last kitten  Star’s ears and fur were singed and she was limping  “She is a brave cat,” said the fireman  “Indeed, she is,” agreed Chloe’s father  “Her screams alerted us all ” “And she is a star all right!” quipped Chloe as she rushed to embrace her cat  The scared kittens snuggled around their mother, as all the neighbours gathered around to have a look at the brave cat'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 218
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MN2F2MciJS0"
      },
      "source": [
        "def finder(s1,s2,s3,s4):\n",
        "  #cleaning\n",
        "  cleanStrys = []\n",
        "  import os\n",
        "  import re\n",
        "\n",
        "  directory = '/content/drive/MyDrive/Colab Notebooks/webpage/KidsStoriesAnimals'\n",
        "\n",
        "  TempM=[]\n",
        "  Temp=[]\n",
        "  StryName=[]\n",
        "  # i=0\n",
        "  for filename in os.listdir(directory):\n",
        "      if filename.endswith(\".txt\"):\n",
        "          TempX=[]\n",
        "          with open(\"/content/drive/MyDrive/Colab Notebooks/webpage/KidsStoriesAnimals\"+'/'+filename,\"r\") as f:\n",
        "            TempX = [line.replace('\\n','') for line in f.readlines() ]\n",
        "            TempX= [  line.split('.') for line in TempX]\n",
        "            TempX= [val for sublist in TempX for val in sublist]\n",
        "            TempX= [  line.split('?') for line in TempX]\n",
        "            TempX= [val for sublist in TempX for val in sublist]\n",
        "            TempX= list(filter(lambda x: x!=' ’ \\n' and x!='\"\\n' and x!='\\n' and x!='’\\n' and x!=' \\n', TempX))\n",
        "            #  TempX= [ l for l in line.split('?') for line in TempX]\n",
        "          TempM.append(TempX)\n",
        "          StryName.append(filename)\n",
        "          continue\n",
        "      else:\n",
        "        continue  \n",
        "\n",
        "  for st in TempM:\n",
        "    single_story = []\n",
        "    for ln in st:\n",
        "      clean_sentence = cleantext.clean(ln, all= True)\n",
        "      single_story.append(clean_sentence)\n",
        "    cleanStrys.append(single_story)\n",
        "\n",
        "\n",
        "  # Cleaning input sentences\n",
        "  inputSentenceFind=[s1,s2,s3,s4]\n",
        "  cleanInput = []\n",
        "  for i in inputSentenceFind:\n",
        "    clean_sentence = cleantext.clean(i, all= True)\n",
        "    cleanInput.append(clean_sentence)\n",
        "\n",
        "\n",
        "\n",
        "  #make each text file into single string\n",
        "  cleanStrysJoined = []\n",
        "  for st in cleanStrys:\n",
        "    str = ''.join(st)\n",
        "    cleanStrysJoined.append(str)\n",
        "    \n",
        "\n",
        "  vectorizer1 = TfidfVectorizer()\n",
        "  maX = -43253535324\n",
        "  maxindx = 0\n",
        "  i = 0\n",
        "  scores = []\n",
        "  print(StryName)\n",
        "  for z in cleanStrysJoined:\n",
        "    \n",
        "    # print('i : ', i)\n",
        "    tfidf = vectorizer1.fit_transform([cleanInput[0], z])\n",
        "    score = ((tfidf * tfidf.T).A)[0,1]\n",
        "    scores.append(score)\n",
        "    # print(score)\n",
        "\n",
        "    if score>maX:\n",
        "      maX = score\n",
        "      maxindx = i\n",
        "\n",
        "    i += 1\n",
        "\n",
        "  print(scores, maX, maxindx)\n",
        "  print('SCORING :: ', StryName[maxindx])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHD11qN0-5s2",
        "outputId": "ebbcf276-7109-4223-d0c1-a13a158c924b"
      },
      "source": [
        "print(options)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['five-wild-dogs.txt', 'the-farmer-and-the-dog.txt']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggwBA7jNCYZy",
        "outputId": "a4d9a911-621a-47da-e637-9cd3bce8d9ca"
      },
      "source": [
        "\n",
        "\n",
        "  # test\n",
        "def cosine_sim(text1, text2):\n",
        "    tfidf = vectorizer.fit_transform([text1, text2])\n",
        "    return ((tfidf * tfidf.T).A)[0,1]\n",
        "\n",
        "\n",
        "print(cosine_sim('Cats eat alot', 'cats sleep alot'))\n",
        "print(cosine_sim('a little bird', 'a little bird chirps'))\n",
        "print(cosine_sim('a little bird', 'a big dog barks'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5031026124151314\n",
            "0.7092972666062738\n",
            "0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuXseJpTRD8i"
      },
      "source": [
        "# Running with Flask"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wMtrTESK6pf"
      },
      "source": [
        "from flask_ngrok import run_with_ngrok\n",
        "from flask import Flask, render_template ,request,url_for,redirect,Response\n",
        "\n",
        "app=Flask(__name__)\n",
        "run_with_ngrok(app)\n",
        "\n",
        "@app.route('/')\n",
        "def home():\n",
        "  return render_template('index.html')\n",
        "  \n",
        "\n",
        "@app.route('/Generated',methods=[\"POST\"])\n",
        "def generate():\n",
        "      ifc1=request.form[\"fact1\"]\n",
        "      ifc2=request.form[\"fact2\"]\n",
        "      ifc3=request.form[\"fact3\"]\n",
        "      ifc4=request.form[\"fact4\"]\n",
        "      animalT=request.form[\"animalName\"]\n",
        "      x=gen(ifc1,ifc2,ifc3,ifc4,animalT)\n",
        "\n",
        "      return render_template('generated.html',f1=x)\n",
        "\n",
        "\n",
        "\n",
        "@app.route('/GPT2',methods=['POST','GET'])\n",
        "def gptGen():\n",
        "  return render_template('/gpt2Gen.html')\n",
        "\n",
        "\n",
        "\n",
        "@app.route('/StryGen',methods=[\"POST\"])\n",
        "def generateGPT():\n",
        "  s1=request.form['s1']\n",
        "  s2=request.form['s2']\n",
        "  s3=request.form['s3']\n",
        "  genreForm=request.form['genreName']\n",
        "  e1=request.form['e1']\n",
        "  e2=request.form['e2']\n",
        "  e3=request.form['e3']\n",
        "  # print(s1,s2,s3,e1,e2,e3,genreForm)\n",
        "  x=genGpt2(s1,s2,s3,'<superhero>',e1,e2,e3)\n",
        "  \n",
        "  return render_template('generated.html',f1=x)\n",
        " \n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "if __name__ =='__main__':\n",
        "  app.run()  "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}